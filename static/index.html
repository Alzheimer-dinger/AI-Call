<!DOCTYPE html>
<html>
<head>
    <title>Call Demo</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; display: flex; flex-direction: column; align-items: center; padding: 20px; background-color: #f0f2f5; color: #1c1e21; }
        h1 { color: #1877f2; }
        #controls { display: flex; width: 90%; max-width: 800px; margin-top: 20px; align-items: center; justify-content: center; }
        #recordButton { padding: 15px 30px; font-size: 18px; cursor: pointer; border: none; border-radius: 50px; background-color: #4CAF50; color: white; transition: background-color 0.3s, box-shadow 0.3s; box-shadow: 0 4px 12px rgba(0,0,0,0.1); }
        #recordButton.recording { background-color: #f44336; }
        #recordButton:hover { box-shadow: 0 6px 16px rgba(0,0,0,0.2); }
        #status { margin-top: 20px; font-size: 16px; color: #606770; }
        #transcriptionContainer { width: 90%; max-width: 800px; margin-top: 30px; background: white; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); padding: 20px; }
        #transcriptionContainer h2 { margin-top: 0; color: #1877f2; font-size: 20px; }
        #transcriptionHistory { max-height: 300px; overflow-y: auto; border: 1px solid #e4e6ea; border-radius: 8px; padding: 15px; background-color: #f8f9fa; }
        .transcript-entry { margin-bottom: 10px; padding: 8px 12px; border-radius: 8px; }
        .transcript-user { background-color: #e3f2fd; border-left: 4px solid #2196f3; }
        .transcript-ai { background-color: #f3e5f5; border-left: 4px solid #9c27b0; }
        .transcript-speaker { font-weight: bold; font-size: 14px; margin-bottom: 4px; }
        .transcript-text { font-size: 15px; line-height: 1.4; }
    </style>
</head>
<body>
    <h1>Call Demo</h1>
    <div id="controls">
        <button id="recordButton">Start Call</button>
    </div>
    <p id="status">Initializing...</p>
    
    <div id="transcriptionContainer">
        <h2>ğŸ“ ì‹¤ì‹œê°„ ì „ì‚¬</h2>
        <div id="transcriptionHistory"></div>
    </div>

    <script>
        const recordButton = document.getElementById('recordButton');
        const status = document.getElementById('status');
        const transcriptionHistory = document.getElementById('transcriptionHistory');
        
        let ws;
        let audioContext;
        let sourceNode;
        let processorNode;
        let audioQueue = [];
        let nextStartTime = 0;
        let isPlaying = false;
        let isRecording = false;
        let accessToken = null;
        
        // ì „ì‚¬ ë²„í¼ë§ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        let lastTranscriptionSpeaker = null;
        let lastTranscriptionEntry = null;
        let transcriptionBuffer = '';
        let transcriptionTimeout = null;

        // Gemini Live ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì˜¤ë””ì˜¤ ì„¤ì •
        const INPUT_SAMPLE_RATE = 16000;  // Gemini ì…ë ¥ ìš”êµ¬ì‚¬í•­
        const OUTPUT_SAMPLE_RATE = 24000; // Gemini ì¶œë ¥ ìƒ˜í”Œ ë ˆì´íŠ¸

        // --- ì „ì‚¬ í‘œì‹œ ë¡œì§ ---
        function addTranscriptionEntry(speaker, text) {
            // íƒ€ì„ì•„ì›ƒ í´ë¦¬ì–´
            if (transcriptionTimeout) {
                clearTimeout(transcriptionTimeout);
                transcriptionTimeout = null;
            }
            
            // ê°™ì€ í™”ìì˜ ì—°ì† ë©”ì‹œì§€ì¸ì§€ í™•ì¸
            if (lastTranscriptionSpeaker === speaker && lastTranscriptionEntry) {
                // ê¸°ì¡´ í…ìŠ¤íŠ¸ì— ì¶”ê°€
                transcriptionBuffer += text;
                
                // ê¸°ì¡´ ì—”íŠ¸ë¦¬ì˜ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                const textContent = lastTranscriptionEntry.querySelector('.transcript-text');
                textContent.textContent = transcriptionBuffer;
                
                // ìë™ ìŠ¤í¬ë¡¤
                transcriptionHistory.scrollTop = transcriptionHistory.scrollHeight;
                
                // 1ì´ˆ í›„ì— ë²„í¼ ì™„ë£Œ ì²˜ë¦¬
                transcriptionTimeout = setTimeout(() => {
                    finishTranscriptionBuffer();
                }, 1000);
                
            } else {
                // ì´ì „ ë²„í¼ ì™„ë£Œ
                finishTranscriptionBuffer();
                
                // ìƒˆë¡œìš´ ì—”íŠ¸ë¦¬ ìƒì„±
                const entry = document.createElement('div');
                entry.className = `transcript-entry transcript-${speaker}`;
                
                const speakerLabel = document.createElement('div');
                speakerLabel.className = 'transcript-speaker';
                speakerLabel.textContent = speaker === 'user' ? 'ğŸ‘¤ ì‚¬ìš©ì' : 'ğŸ¤– AI';
                
                const textContent = document.createElement('div');
                textContent.className = 'transcript-text';
                textContent.textContent = text;
                
                entry.appendChild(speakerLabel);
                entry.appendChild(textContent);
                
                transcriptionHistory.appendChild(entry);
                
                // ìƒˆë¡œìš´ ë²„í¼ ì‹œì‘
                lastTranscriptionSpeaker = speaker;
                lastTranscriptionEntry = entry;
                transcriptionBuffer = text;
                
                // ìë™ ìŠ¤í¬ë¡¤
                transcriptionHistory.scrollTop = transcriptionHistory.scrollHeight;
                
                // 1ì´ˆ í›„ì— ë²„í¼ ì™„ë£Œ ì²˜ë¦¬
                transcriptionTimeout = setTimeout(() => {
                    finishTranscriptionBuffer();
                }, 1000);
            }
        }
        
        function finishTranscriptionBuffer() {
            if (transcriptionTimeout) {
                clearTimeout(transcriptionTimeout);
                transcriptionTimeout = null;
            }
            lastTranscriptionSpeaker = null;
            lastTranscriptionEntry = null;
            transcriptionBuffer = '';
        }

        // --- PCM ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë¡œì§ ---
        function initializeAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                console.log(`AudioContext initialized with sample rate ${audioContext.sampleRate}`);
            }
        }

        // Float32Arrayë¥¼ 16-bit PCMìœ¼ë¡œ ë³€í™˜
        function float32ToPCM16(float32Array, targetSampleRate) {
            const sourceSampleRate = audioContext.sampleRate;
            let resampledData = float32Array;

            // ë¦¬ìƒ˜í”Œë§ì´ í•„ìš”í•œ ê²½ìš°
            if (sourceSampleRate !== targetSampleRate) {
                const ratio = sourceSampleRate / targetSampleRate;
                const newLength = Math.round(float32Array.length / ratio);
                resampledData = new Float32Array(newLength);

                for (let i = 0; i < newLength; i++) {
                    const sourceIndex = Math.round(i * ratio);
                    resampledData[i] = float32Array[Math.min(sourceIndex, float32Array.length - 1)];
                }
            }

            // Float32ë¥¼ 16-bit PCMìœ¼ë¡œ ë³€í™˜
            const pcm16 = new Int16Array(resampledData.length);
            for (let i = 0; i < resampledData.length; i++) {
                const sample = Math.max(-1, Math.min(1, resampledData[i]));
                pcm16[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            }

            return pcm16.buffer;
        }

        async function schedulePlayback() {
            if (isPlaying || audioQueue.length === 0) {
                return;
            }
            isPlaying = true;

            const audioData = audioQueue.shift(); // íì—ì„œ ì²« ë²ˆì§¸ ì²­í¬ ê°€ì ¸ì˜¤ê¸°

            // Gemini LiveëŠ” 24kHz 16-bit PCMìœ¼ë¡œ ì‘ë‹µì„ ë³´ëƒ„
            const pcm16Data = new Int16Array(audioData);
            const float32Data = new Float32Array(pcm16Data.length);

            // 16-bit PCMì„ Float32ë¡œ ë³€í™˜ (ì •ê·œí™”)
            for (let i = 0; i < pcm16Data.length; i++) {
                float32Data[i] = pcm16Data[i] / 32767.0;
            }

            // AudioBuffer ìƒì„± (Gemini ì¶œë ¥ ìƒ˜í”Œ ë ˆì´íŠ¸ ì‚¬ìš©)
            const audioBuffer = audioContext.createBuffer(1, float32Data.length, OUTPUT_SAMPLE_RATE);
            audioBuffer.getChannelData(0).set(float32Data);

            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);

            const currentTime = audioContext.currentTime;
            const startTime = Math.max(currentTime, nextStartTime);
            
            source.start(startTime);
            
            // ë‹¤ìŒ ì²­í¬ì˜ ì‹œì‘ ì‹œê°„ì„ í˜„ì¬ ì²­í¬ì˜ ê¸¸ì´ë§Œí¼ ë’¤ë¡œ ì„¤ì •
            nextStartTime = startTime + audioBuffer.duration;
            
            source.onended = () => {
                isPlaying = false;
                schedulePlayback(); // ì¬ê·€ì ìœ¼ë¡œ ë‹¤ìŒ ì²­í¬ ì¬ìƒ ìŠ¤ì¼€ì¤„ë§
            };
        }

        // --- WebSocket ë° ë…¹ìŒ ë¡œì§ ---
        async function getAccessToken() {
            try {
                const response = await fetch("/api/auth/token", { method: "POST" });
                if (!response.ok) throw new Error(`Token fetch failed: ${response.statusText}`);
                const data = await response.json();
                accessToken = data.access_token;
                status.textContent = "Ready. Press 'Start Call'.";
                recordButton.disabled = false;
            } catch (error) {
                console.error("Authentication error:", error);
                status.textContent = "Authentication failed. Please refresh.";
            }
        }

        function setupWebSocket() {
            if (!accessToken) return;
            const wsUrl = `ws://localhost:8000/api/ws/live?token=${accessToken}`;
            ws = new WebSocket(wsUrl);
            ws.binaryType = 'arraybuffer';

            ws.onopen = () => startRecordingStream();
            ws.onmessage = (event) => {
                if (event.data instanceof ArrayBuffer) {
                    console.log(`Received ${event.data.byteLength} audio bytes from backend.`);
                    audioQueue.push(event.data);
                    schedulePlayback();
                } else {
                    // í…ìŠ¤íŠ¸ ë©”ì‹œì§€ (ì „ì‚¬ ë°ì´í„°) ì²˜ë¦¬
                    try {
                        const transcriptionData = JSON.parse(event.data);
                        if (transcriptionData.type === 'transcription') {
                            addTranscriptionEntry(transcriptionData.speaker, transcriptionData.text);
                        }
                    } catch (error) {
                        console.error('Error parsing transcription data:', error);
                    }
                }
            };
            ws.onclose = () => stopRecordingStream(false);
            ws.onerror = (error) => console.error("WebSocket error:", error);
        }

        async function startRecordingStream() {
            if (!navigator.mediaDevices?.getUserMedia) return alert('Audio recording not supported.');
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: INPUT_SAMPLE_RATE,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });

                sourceNode = audioContext.createMediaStreamSource(stream);

                // ScriptProcessorNode ì‚¬ìš© (êµ¬í˜•ì´ì§€ë§Œ ë” ì•ˆì •ì )
                const bufferSize = 4096;
                processorNode = audioContext.createScriptProcessor(bufferSize, 1, 1);

                processorNode.onaudioprocess = (event) => {
                    if (ws?.readyState === WebSocket.OPEN) {
                        const inputBuffer = event.inputBuffer.getChannelData(0);
                        const pcmData = float32ToPCM16(inputBuffer, INPUT_SAMPLE_RATE);
                        ws.send(pcmData);
                    }
                };

                sourceNode.connect(processorNode);
                processorNode.connect(audioContext.destination);

                isRecording = true;
                recordButton.textContent = "Stop Call";
                recordButton.classList.add('recording');
                status.textContent = "I'm listening...";

            } catch (err) {
                console.error("Microphone error:", err);
                status.textContent = "Microphone access failed.";
            }
        }

        function stopRecordingStream(closeWebSocket = true) {
            if (processorNode) {
                processorNode.disconnect();
                processorNode = null;
            }
            if (sourceNode) {
                sourceNode.disconnect();
                sourceNode = null;
            }

            // ì „ì‚¬ ë²„í¼ ì •ë¦¬
            finishTranscriptionBuffer();

            isRecording = false;
            recordButton.textContent = "Start Call";
            recordButton.classList.remove('recording');
            status.textContent = "Call ended.";

            if (closeWebSocket && ws?.readyState === WebSocket.OPEN) ws.close();
        }

        recordButton.addEventListener('click', () => {
            // ì‚¬ìš©ìì˜ ì²« ìƒí˜¸ì‘ìš© ì‹œ AudioContextë¥¼ ì´ˆê¸°í™” (ë¸Œë¼ìš°ì € ìë™ì¬ìƒ ì •ì±…)
            initializeAudioContext();
            if (isRecording) {
                stopRecordingStream();
            } else {
                setupWebSocket();
            }
        });

        // Initial setup
        recordButton.disabled = true;
        getAccessToken();

    </script>
</body>
</html>
